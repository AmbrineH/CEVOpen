<?xml version="1.0" encoding="UTF-8"?>
<p class="p">Different architectures were explored with a different number of hidden neurons and two transfer functions for the hidden layers (hyperbolic tangent sigmoid and logistic sigmoid transfer functions). The linear transfer function for output layer neurons was used (see ii. in 
 <xref ref-type="fig" rid="sensors-18-01922-f002" class="xref">Figure 2</xref>). Two training algorithms (Levenbergâ€“Marquardt (LM) and resilient backpropagation (RP) [
 <xref rid="B44-sensors-18-01922" ref-type="bibr" class="xref">44</xref>]) were explored. The goal function to update the parameters of the network was the least squares error (see iii. in 
 <xref ref-type="fig" rid="sensors-18-01922-f002" class="xref">Figure 2</xref>).
</p>
